# ğŸ§  MCP Chat with GPT Streaming

A full-stack real-time chat app powered by:

- **Centrifugo** for realtime message delivery
- **FastAPI** (async) backend for handling chat and AI requests
- **OpenAI** for LLM responses
- **Streaming token delivery** via temporary Centrifugo channels using the MCP (Message Communication Protocol)

---

## ğŸš€ Features

- âœ… Realtime chat using Centrifugo
- âœ… `/ask` questions sent to OpenAI and streamed token-by-token
- âœ… Frontend subscribes to a unique UUID-based stream channel per question
- âœ… Stream ends cleanly with a `done: true` signal
- âœ… Fully MCP-compliant message format

---

## ğŸ§° Tech Stack

- **FastAPI** â€“ async backend with streaming support
- **Centrifugo** â€“ pub/sub websocket server
- **OpenAI API** â€“ LLM responses (via GPT-3.5/4)
- **httpx** â€“ async publishing to Centrifugo
- **Docker Compose** â€“ one command to run everything

---

## ğŸ—‚ Directory Structure

```
project-root/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app.py # FastAPI backend with streaming
â”‚ â”œâ”€â”€ Dockerfile
â”‚ â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚ â””â”€â”€ index.html # HTML UI using Centrifuge.js
â”œâ”€â”€ centrifugo/
â”‚ â””â”€â”€ config.json # Centrifugo config
â”œâ”€â”€ .env # Your API keys (gitignored)
â”œâ”€â”€ docker-compose.yml
```


---

## âš™ï¸ Setup

1. Copy `.env.example` to `.env` and fill in:

```env
CENTRIFUGO_API_KEY=your_centrifugo_api_key
OPENAI_API_KEY=your_openai_key
```

Then:

```
docker compose up --build
```

Visit: http://localhost:3000

## ğŸ§ª Example

Type:

```bash
/ask What is the capital of Japan?
```

Watch GPTBot stream:

```bash
GPTBot: The capital of Japan is Tokyo.
```
